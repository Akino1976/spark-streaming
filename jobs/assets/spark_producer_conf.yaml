properties:
    __anchors__:
        spark_conf: &spark_conf
            - !!python/tuple ["spark.hadoop.fs.s3a.path.style.access", "true"]
            - !!python/tuple ["spark.hadoop.fs.s3a.impl", "org.apache.hadoop.fs.s3a.S3AFileSystem"]
            - !!python/tuple ["spark.driver.extraJavaOptions", "-Dcom.amazonaws.services.s3.enableV4=true"]
            - !!python/tuple ["spark.hadoop.mapreduce.fileoutputcommitter.algorithm.version", 2]
            - !!python/tuple ["spark.speculation", "false"]
            - !!python/tuple ["spark.sql.execution.arrow.pyspark.enabled", "true"]
            - !!python/tuple ["spark.sql.sources.partitionOverwriteMode", "dynamic"]
            - !!python/tuple ["spark.streaming.dynamicAllocation.enabled", "true"]
            - !!python/tuple ["spark.sql.debug.maxToStringFields", 1000]
            - !!python/tuple ["spark.sql.shuffle.partitions", 100]
            - !!python/tuple ["spark.sql.parquet.mergeSchema", "false"]
            - !!python/tuple ["spark.hadoop.parquet.enable.summary-metadata", "false"]
            - !!python/tuple ["spark.serializer", "org.apache.spark.serializer.KryoSerializer"]
    docker:
        - !!python/tuple ["spark.hadoop.fs.s3a.impl", "org.apache.hadoop.fs.s3a.S3AFileSystem"]
        - !!python/tuple ["spark.hadoop.fs.s3a.aws.credentials.provider", "org.apache.hadoop.fs.s3a.TemporaryAWSCredentialsProvider"]
        - !!python/tuple ["spark.hadoop.fs.s3a.access.key", "AAAAAAAAAAAAAAAAAAAA"]
        - !!python/tuple ["spark.hadoop.fs.s3a.secret.key", "aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa"]
        - !!python/tuple ["spark.hadoop.fs.s3a.session.token", "token"]
        - !!python/tuple ["spark.sql.parquet.compression.codec", "snappy"]
        - !!python/tuple ["spark.sql.shuffle.partitions", 100]
        - !!python/tuple ["spark.sql.adaptive.enabled", "true"]
        - !!python/tuple ["spark.io.compression.codec", "gzip"]
        - !!python/tuple ["spark.sql.sources.partitionOverwriteMode", "dynamic"]
        - !!python/tuple ["spark.driver.extraJavaOptions", "-Dcom.amazonaws.services.s3.enableV4 -Dcom.amazonaws.services.s3.enforceV4"]
        - !!python/tuple ["spark.executor.extraJavaOptions", "-Dcom.amazonaws.services.s3.enableV4 -Dcom.amazonaws.services.s3.enforceV4"]
        - !!python/tuple ["spark.sql.catalog.iceberg_catalog.glue.skip-name-validation", "false"]
        - !!python/tuple ["spark.hadoop.mapreduce.fileoutputcommitter.algorithm.version", 2]
        - !!python/tuple ["spark.sql.parquet.compression.codec", "snappy"]
        - !!python/tuple ["spark.sql.debug.maxToStringFields", 1000]
        - !!python/tuple ["spark.streaming.dynamicAllocation.enabled", "true"]
        - !!python/tuple ["spark.sql.parquet.mergeSchema", "false"]
        - !!python/tuple ["spark.hadoop.parquet.enable.summary-metadata", "false"]
        - !!python/tuple ["spark.serializer", "org.apache.spark.serializer.KryoSerializer"]
    staging: *spark_conf
    production: *spark_conf
